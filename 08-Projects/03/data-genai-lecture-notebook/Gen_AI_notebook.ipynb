{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generative AI notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to show off some of the many different ways of leveraging the power of advanced models like OpenAI's ChatGPT through their APIs rather than simply a web interface.\n",
    "\n",
    "#### Requirements \n",
    "\n",
    "As this field is evolving at an extremely rapid pace (e.g. OpenAI has only recently deprecated several of their model endpoints), ensuring stability with such tools can be tricky. The following package versions below, __when run in a Colab environment__, yield consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sth4Z_Z9Q1zD"
   },
   "outputs": [],
   "source": [
    "! pip install openai==0.28 \\\n",
    "langchain==0.0.345 \\\n",
    "pypdf==3.17.4 \\\n",
    "chromadb==0.4.22 \\\n",
    "tiktoken==0.5.2 \\\n",
    "huggingface_hub==0.19.4 \\\n",
    "diffusers==0.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Towards the end of this notebook, local inference is run on a highly quantized version of Meta's LLama 2 model. The weights required to perform this are large (~4GB) so downloading them from HuggingFace before you attempt the rest of the notebook will save waiting later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf \\\n",
    "        --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you will need an OpenAI API key to run many of the below examples. You can sign up for one [here](https://openai.com/blog/openai-api) and should be able to get a free trial if you are a first time user. Even if you are not, the examples presented here will cost only a few cents to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = 'your-openai-api-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yvse2CzgQLMh",
    "outputId": "65ec239e-5285-4482-cca2-bf4416396098"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI API client and set your API key\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsEorYryQS4F",
    "outputId": "a557b24e-2687-440b-d8ff-3098bc6baccb"
   },
   "outputs": [],
   "source": [
    "# Prompt for the AI model\n",
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Make a request to the API to generate text\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",  # Use the engine of your choice\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens = 50\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RluOi0u2QYc2",
    "outputId": "d660e0bc-1aa0-45d3-9d9a-f1caf07c015f"
   },
   "outputs": [],
   "source": [
    "# Prompt for the AI model\n",
    "prompt = \"Give instructions to cook vegetable samosas\"\n",
    "\n",
    "# Make a request to the API to generate text\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",  # Use the engine of your choice\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a sassy culinary instructor that gives sarcastic replies\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens = 50\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CNE9ePH2Qbga",
    "outputId": "fcb2f8d8-15f3-4194-c350-1ac2d52abf6c"
   },
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I'm interested in the weather in Bozeman. I'm old-school so like it in F?\"}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city with its accompanying state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\",\n",
    "                         \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call=\"auto\",\n",
    ")\n",
    "completion[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A worked example leveraging OpenAI and a local DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hEU5T1NQe77"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/results.csv\")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we descrive a function that might be used to query our DataFrame. Feel free to change the `\"user\"` prompt in the `messages` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPaWXnWkQguY"
   },
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about matches that took place in Italy between 1980 up until the end of the 20th century\"}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_matches\",\n",
    "        \"description\": \"Return the rows in a DataFrame about women's football games which satisfy the criteria\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"country\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the country the matches took place e.g. France or China\",\n",
    "                },\n",
    "                \"start_year\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The year to begin filtering from e.g. 1956\",\n",
    "                },\n",
    "                \"end_year\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The year to end filtering on e.g. 2005\"}\n",
    "            },\n",
    "            \"required\": [\"location\", \"start_year\", \"end_year\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the response to something we can pass into a locally defined function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaQ_GsVWQi3n"
   },
   "outputs": [],
   "source": [
    "args = json.loads(completion[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually defining the local function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaQ_GsVWQi3n"
   },
   "outputs": [],
   "source": [
    "def matches_finder(country: str, start_year: int, end_year: int):\n",
    "    return df.loc[\n",
    "        (df[\"country\"] == country) &\n",
    "        (start_year <= df[\"date\"].dt.year) &\n",
    "        (df[\"date\"].dt.year <= end_year)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using arguments from our OpenAI Function call to interact with our locally defined function/ DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_finder(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with embeddings and larger documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KIxd3ZoyQlJ_",
    "outputId": "840d54e5-5af5-4be2-a527-c559589a57da"
   },
   "outputs": [],
   "source": [
    "# Creating embeddings\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "embedding = openai.Embedding.create(input = [\"\"\"This is a simple embedding of a sentence\"\"\"],\n",
    "                                    model = model)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.array(embedding[\"data\"][0][\"embedding\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we download a book in PDF form that we can then use Langchain's document loader to prepare it for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "! wget -O book.pdf \"https://greenteapress.com/thinkpython2/thinkpython2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"book.pdf\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with a large document, we need to split it into smaller chunks with one of Langchain's `text_splitter`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (f'You have {len(data)} documents in your data')\n",
    "print (f'''There are ~{np.mean([len(x.page_content) for x in data])} characters per document''')\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we embed our documents directly into an in-memory vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "vector_db = Chroma.from_documents(texts, \n",
    "                                  OpenAIEmbeddings(openai_api_key = openai_api_key, \n",
    "                                                   model=\"text-embedding-ada-002\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then embed a sentence (e.g. a question) and see which of our texts are most similar to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "# Querying the data\n",
    "query = \"How do I establish a Class?\"\n",
    "num_closest_docs = 5\n",
    "docs = vector_db.similarity_search(query, k = num_closest_docs)\n",
    "for k in range(num_closest_docs):\n",
    "    print(f\"\"\"\\n ~~~~~ Showing document #{k+1} ~~~~~ \\n\"\"\")\n",
    "    print(docs[k].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can go further, passing this retrieved text as context for a prompt which we can then do question-answering on. Using `verbose = True` will allow you to see the chain of events taking place under the hood.\n",
    "\n",
    "With Langchain, these pre-defined prompts can be altered for whatever purpose necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = OpenAI(temperature=0, \n",
    "             openai_api_key=openai_api_key, \n",
    "             model = \"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "chain = load_qa_chain(llm, \n",
    "                      chain_type=\"map_reduce\",\n",
    "                     verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxglzYZUQq7p",
    "outputId": "ff6371a2-80e1-4013-8120-0602abff9c9f"
   },
   "outputs": [],
   "source": [
    "query = \"How do I define a class in Python\"\n",
    "\n",
    "docs = vector_db.similarity_search(query, \n",
    "                                  k = 1)\n",
    "print(docs)\n",
    "print(chain.run(input_documents=docs, question=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running large LLMs locally w/ quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run these cells, __Colab with a GPU enabled is strongly recommended__, as it will significantly speed up inference times. What we are doing here is taking a version of Meta's Llama 2 model that has been significantly reduced in size and running inference on it entirely locally (simply by loading its weights onto a GPU)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ctransformers[cuda]>=0.2.24 # for Colab\n",
    "# ! CT_METAL=1 pip install ctransformers==0.2.27 --no-binary ctransformers # For Apple Metal devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide the path to the model's location locally, assuming it is in the same directory as the notebook - if running this in Colab you will need the `/content/llama...` in your path. If running elsewhere, you will not need the `/content/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WxC2NEJPlle",
    "outputId": "89a7dc53-47a9-4868-a48b-c64ea68eef3b"
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"/content/llama-2-7b-chat.Q4_K_M.gguf\", # Remove /content/ as needed\n",
    "                                           model_file=\"llama-2-7b-chat.q4_K_M.gguf\", \n",
    "                                           model_type=\"llama\", \n",
    "                                           gpu_layers = 50)\n",
    "\n",
    "print(llm(\"Q: What is the size of the earth's diamter? A:\", max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we demonstrate usage of Stable Diffusion - an open source alternative to the likes of Dall-E 2 and MidJourney. Again, Colab w/ GPU is strongly recommended for faster inference here. \n",
    "\n",
    "See the comments about using `torch.float32` and `.to(\"cuda\")` for implementations without GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752,
     "referenced_widgets": [
      "e44fde7389e64a18b691315efb185a03",
      "48e894c4a55b4ff9b7a3552d2eb5611b",
      "e3d4e844ace54d04ad2036ff2825596c",
      "2f337a1f230d4944ba79144bc0742ce0",
      "3512510e5a8b46cc982b823549265ca7",
      "79eab1f7332c45c7bfd134831e523405",
      "1bcc2c2e7c584200acfc9ff34980ba61",
      "f3f1bc70f33e455ebe49e8ccb92208d4",
      "97be2b4a07bf4ffd8c2347ce6077934e",
      "c88dd056b9994579a9c5b65fa08e4e76",
      "146df8c3162d4408882aa88fdb563277",
      "4e84349fe7424607a194e0f2f389d91b",
      "7edecc5c0e9c4824997140da4455c833",
      "48bf1ad4851d4a33a3e98426e6035776",
      "bad3d66cda8646d48476a52b1b7ba261",
      "94a0d03b23504a37b8995569b965d65b",
      "a47161ffec0e414ca8f9c71215c27740",
      "7cea548829b542cea627600896562d7d",
      "c0bc67a7062142619ab715d9d3f79de8",
      "c8839aa1d9d9431e97c90f9a4b15d113",
      "190e1b66fa4c45b7924e38bcdd9fd603",
      "e126d712d8284abd8297e2d14b0d6fd1"
     ]
    },
    "id": "Dp96lXq5TQtI",
    "outputId": "e2e0f459-3c84-4769-ab28-30b8e02237a9"
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", \n",
    "    torch_dtype=torch.float16, # Change to float32 if running without GPU, float 16 for GPU\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\") # If not using a GPU, remove the .to(\"cuda\")\n",
    "\n",
    "prompt = \"A Renaissance painting of the Eiffel tower\" # The prompt can be changed here\n",
    "pipeline(prompt, num_inference_steps=30).images[0] # Change the number of inference steps for variations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
